---
title: "Practical Machine Learning - Case Study"
knit: (function(input_file, encoding) {out_dir <- 'docs'; rmarkdown::render(input_file,encoding=encoding,output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Michael Louis Pastor"
date: "1/11/2022"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#====================================================================
#  Load the Packages
#
library(caret)
library(dplyr)
library(ggplot2)
library(stringr)

library(AppliedPredictiveModeling)
library(ElemStatLearn)
library(rpart)        # Recursive partition trees
library(randomForest) # Random forest
library(gbm)          # Boosting

library( quantmod )   # forecasting data
library( forecast )   # timeseries forecasting data

```
##   Just how good was that workout?

As part of the excellent Applied Machine Learning course at Johns Hopkins University, we are examining the quality of physical workouts.  

The UCI Machine Learning Repository ( [See them here](https://archive.ics.uci.edu/ml/datasets/Wearable+Computing%3A+Classification+of+Body+Postures+and+Movements+(PUC-Rio) ) ) has generously provided a significant dataset which includes physical measurements from workouts.  Their "Wearable Computing" measurements are invaluable for this field of research. 

##   Assignment Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 


##   Goals & Deliverables

Our goal is to predict the manner in which they did the exercise.  This is the **"classe"** variable in the pml-training dataset. It has values from character 'A' through 'E' to denote the quality of the workout.  We will transform this into a five level Factor for our models.

Please see the R Markdown source code and supporting files at our Git Site ( [Here in GitHub]( https://github.com/mikepastor12/JohnsHopkins-CaseStudy-2022)  )


Let's take a closer look at "classe" which is our Dependent/Response/Y variable...


```{r, include=FALSE, echo=FALSE}

rm( list = ls() )    # Clear Environment objects

# Open the data files
# 
DATASETPATH <- "./"
list.files(path = DATASETPATH)

myFilename <- paste( DATASETPATH, "pml-training.csv", sep="" )
pml_training <- read.csv(myFilename)
nrow( pml_training )

myFilename <- paste( DATASETPATH, "pml-testing.csv", sep="" )
pml_testing <- read.csv(myFilename)
nrow( pml_testing )

print( "PML Data Files have been Loaded  ")


#========================================================================
#     Create the 'training' and 'testing' datasets from the 
#       'pml-training.csv' dataset.
#     This allows to perform simulated 'out of sample' measurements
#      to measure the progress of our model.
#
inTrain = createDataPartition(pml_training$classe, p =0.67, list=FALSE )
training = pml_training[inTrain,]
testing = pml_training[-inTrain,]

print( "Datset has been partitioned  ")
#==================================================================
#  Cleanse the data
#

# Makes sure that 'classe' is a factor
training$classe  <- as.factor(training$classe)
summary( training$classe )

# new_window and user_name  should also be Factors
training$new_window  <- as.factor(training$new_window)
training$user_name  <- as.factor(training$user_name)

 
```

```{r fig.align='center', fig.height=3, fig.width=4 }

summary( training$classe )
plot( training$classe )

```

## Which predictor variables should we use?

Let's take a closer look at our Independent/Explanatory/X variables by fitting a Regularized Regression (LASSO) model.

This will give us guidance on the most valuable predictor variables in the dataset.

Twelve of the columns in the Training dataset contain division by zero errors - let's remove them on the model training datsets...


```{r include=FALSE, echo=FALSE}

#  
# These columns contain a Division by Zero error in the original dataset.
#   Let's remove them for the model building exercises.
#
omitNA_columns <- c(-130,-127, -92, -89, -74, -73, -71, -70, -17, -16, -14, -13 )

cleanTrainingDF <- training[, omitNA_columns]
cleanTestingDF <- testing[, omitNA_columns]

#  Use a LASSO - Regularized Regression model
#
myLASSO <-  train( classe ~ ., 
                   data=cleanTrainingDF, 
                   preProcess=c("center", "scale"),
                   method="glmnet",
                   na.action = na.exclude )
date()
print( myLASSO )



```
The trained LASSO Model now can give us the relevant fields.

Let's take the predictors with an Overall impact greater than 7. Exclude the timestamps and outliers.

Here are our candidates!  

```{r echo=FALSE }


# Use varImp to access the Cofficients and show us the best predictors
#
myVarList  <-  varImp( myLASSO )
myDF <- as.data.frame( myVarList$importance )
myDF$varName <- rownames( myDF )

myDF$OverallImp <-  myDF$A + myDF$B + myDF$C + myDF$D + myDF$E 
bestPredictorsDF <- myDF %>%
  filter( OverallImp > 6 & OverallImp < 100 ) %>%
  filter( ! str_detect( varName, '^cvtd_timestamp' ))  %>%
  arrange(OverallImp) 

bestPredictorsDF


```


Let's use these predictor columns to build a Random Forest on the dataset and test our prediction accuracy...


```{r  echo=FALSE}

tmpStr <- paste(bestPredictorsDF$varName, collapse=" + " )
formula_str <- paste( "classe ~ ", tmpStr)
# formula( formula_str )
# 

#  Fit a Random forest 
myRF <-  train(
  form=formula( formula_str ),
                data=cleanTrainingDF, method="rf",
                prox=TRUE,
                na.action = na.exclude  )
print( myRF )

# Prediction
pred2 <- predict( myRF, newdata=cleanTestingDF )

# trim testing to suit the prediction length
pCleanTestingDF <- cleanTestingDF[1:length(pred2),]
# 
# nrow( pCleanTestingDF )
# length( pred2 )

pCleanTestingDF$predictedRight2 <- pred2 == pCleanTestingDF$classe


# view the Cross table and also graph the ACCURACY 
print( "Our Out-Of-Sample Accuracy Crosstab below" )
table( pred2, pCleanTestingDF$classe)


#  Accuracy percentage 
accuracyRate <-
  nrow( pCleanTestingDF[ which( pCleanTestingDF$predictedRight2 ==TRUE ), ] ) /
  nrow( pCleanTestingDF )
# 

paste( "Our Out-Of-Sample Accuracy rate is -->   ", round( accuracyRate, 4)  )


```




